---
title: "far: a (f)ast re-write of the Unix utility p(ar)"
permalink: /posts/far/
---

# A (f)ast re-write of p(ar) - far

[[Video Guide]](https://youtu.be/H3Agto3ZSnk)
[[Source Code (C++)]](./far.cpp) [[Source Code (Python)]](./far.py)

[`par`](http://www.nicemice.net/par/) is a formatting tool that inserts line
breaks to make the length of each line less than a set number of characters,
usually 79 (terminals historically have 80 width). Unfortunately, `par` is
incredibly complicated and introduces random whitespace. So I made my own.

For `far` to make the paragraphs look good, it minimizes the variance of each
line. However, itâ€™s constrained to use no more than the optimal number of
lines, so it doesn't generate really short lines. Finally, it ignores the
last line when minimizing variance and tries to make the last line shorter
than average, because a typical paragraph usually looks better if the last
line is shorter. To summarize,
1. Minimize the variance of the lengths of each line...
2. ... subject to the constraint that the number of lines is optimal
3. Ignore the last line, while making sure it's shorter than average

`far` uses dynamic programming to minimize variance. It
tokenizes the paragraph by splitting on whitespace, and each
subproblem in the dynamic program is a suffix of this token list.
```python
Var[X] = E[X]^2 - E[X]^2 = sum(x^2 for x in X)/len(X) - (sum(X)/len(X))^2.
```
The length `len(X)` is constant because of the
optimal number of lines constraint, and so is the sum because the sum of the
line lengths is determined by two things: the characters in the tokens and
the number of spaces introduced by merging two tokens (combining the words
"hello" and "world" onto the same line gives "hello world", with an additional
space). The characters stay the same, and the number of spaces is fixed if the
number of lines is fixed. Each token starts off as its own line, and each merge
reduces the number of lines by 1, so if two solutions have the same number of
lines they must have done the same number of merges. 

Thus, minimizing `Var[X]` is equivalent to minimizing the sum of squares
`sum(x^2 for x in X)` if the number of lines is fixed. Recall that we
are trying to minimize variance over the entire paragraph. The overall
paragraph has some mean value u. Each line will contribute `(x - u)^2`
to the overall paragraph's variance. So we want to minimize:
```
(x1 - u)^2 + (x2 - u)^2 + ... + (xn - u)^2
```
where xi is the length of a line and we know that `x1 + x2 + ... + xn` is
constant because of the above logic (`sum(X)` is constant). Expanding,
```
[x1^2 - 2u x1 + u^2] + [x2^2 - 2u x2 + u^2] + ... + [xn^2 - 2u xn + u^2]
u^2 is a constant, so we can discard those terms and reorganize into
[x1^2 + x2^2 + ... + xn^2] - 2u[x1 + x2 + ... + xn]. 
```
The last term is a constant, so minimizing the variance of the overall
paragraph is equivalent to minimizing the variance for a suffix of the
paragraph (both are minimizing the sum of squares). This is just the variance
of the subproblem, so the dynamic programming is valid since optimal
substructure holds. In practice, I skip calculating variance entirely and
simply minimize the sum of squares. I also do dynamic programming on the
variance of each _prefix_, so that I can easily ignore the last line.

That's it! Runs in `O(NK)` where N = # characters and K = width.

## Examples

`par`: usually pretty good, but here it
introduces a weird spike on the penultimate line:
(note: for some reason, `par` now works on this example.
TODO: find an example where `par` fails)
```
In this chapter we will be using the MNIST dataset, which is a set of 70,000
small images of digits handwritten by high school students and employees of the
US Cen- sus Bureau. Each image is labeled with the digit it represents. This
set has been stud- ied so much that it is often called the "hello world"
of Machine Learning: whenever people come up with a new classification
algorithm they are curious to see how it will perform on MNIST, and anyone who
learns Machine Learning tackles this dataset sooner or later.
```

`far`: note the improved consistency.
```
In this chapter we will be using the MNIST dataset, which is a set of 70,000
small images of digits handwritten by high school students and employees of the
US Cen- sus Bureau. Each image is labeled with the digit it represents. This
set has been stud- ied so much that it is often called the "hello world" of
Machine Learning: whenever people come up with a new classification algorithm
they are curious to see how it will perform on MNIST, and anyone who learns
Machine Learning tackles this dataset sooner or later.
```

## Uses

This program is pretty useful whenever writing plaintext in a monospace text
editor, e.g. when editing LaTeX, markdown files, or college essays. It's
especially useful in `vim`, which lets you set the option `'formatprg'` so
the operator `gq` formats using an external program.

